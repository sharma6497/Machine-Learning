{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c2bdc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\10664440\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\10664440\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing the libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "#Download the following modules once\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94423015",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the training set\n",
    "train_data = pd.read_excel(\"Data_Train01.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39912075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               STORY  SECTION\n",
      "0  But the most painful was the huge reversal in ...        3\n",
      "1  How formidable is the opposition alliance amon...        0\n",
      "2  Most Asian currencies were trading lower today...        3\n",
      "3  If you want to answer any question, click on ‘...        1\n",
      "4  In global markets, gold prices edged up today ...        3\n"
     ]
    }
   ],
   "source": [
    "#Printing the top 5 rows\n",
    "print(train_data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d35ca4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7628 entries, 0 to 7627\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   STORY    7628 non-null   object\n",
      " 1   SECTION  7628 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 119.3+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Printing the dataset info\n",
    "print(train_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb7b702b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7628, 2)\n"
     ]
    }
   ],
   "source": [
    "#Printing the shape of the dataset\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1eda75b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">STORY</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SECTION</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1686</td>\n",
       "      <td>1673</td>\n",
       "      <td>This story has been published from a wire agen...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2772</td>\n",
       "      <td>2731</td>\n",
       "      <td>This story has been published from a wire agen...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1924</td>\n",
       "      <td>1914</td>\n",
       "      <td>We will leave no stone unturned to make the au...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1246</td>\n",
       "      <td>1233</td>\n",
       "      <td>This story has been published from a wire agen...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        STORY                                                               \n",
       "        count unique                                                top freq\n",
       "SECTION                                                                     \n",
       "0        1686   1673  This story has been published from a wire agen...    4\n",
       "1        2772   2731  This story has been published from a wire agen...   13\n",
       "2        1924   1914  We will leave no stone unturned to make the au...    3\n",
       "3        1246   1233  This story has been published from a wire agen...   11"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Printing the group by description of each category\n",
    "train_data.groupby(\"SECTION\").describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d0a0ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing duplicates to avoid overfitting\n",
    "train_data.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a42669d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A punctuations string for reference (added other valid characters from the dataset)\n",
    "all_punctuations = string.punctuation + '‘’,:”][],' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86dcc97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method to remove punctuation marks from the data\n",
    "def punc_remover(raw_text):\n",
    "    no_punct = \"\".join([i for i in raw_text if i not in all_punctuations])\n",
    "    return no_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab71e05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method to remove stopwords from the data\n",
    "def stopword_remover(no_punc_text):\n",
    "    words = no_punc_text.split()\n",
    "    no_stp_words = \" \".join([i for i in words if i not in stopwords.words('english')])\n",
    "    return no_stp_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "916cf5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method to lemmatize the words in the data\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "def lem(words):\n",
    "    return \" \".join([lemmer.lemmatize(word,'v') for word in words.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22d8c18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method to perform a complete cleaning\n",
    "def text_cleaner(raw):\n",
    "    cleaned_text = stopword_remover(punc_remover(raw))\n",
    "    return lem(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a84cec03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi sample text test text cleaner method Removes special character stopwords And lemmatizes go go run run run'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing the cleaner method\n",
    "text_cleaner(\"Hi!, this is a sample text to test the text cleaner method. Removes *@!#special characters%$^* and stopwords. And lemmatizes, go, going - run, ran, running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "adbe3730",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=train_data[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63f0f99a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9593950d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying the cleaner method to the entire data\n",
    "train_data['CLEAN_STORY'] = train_data['STORY'].apply(text_cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9b9833e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the new dataset\n",
    "#print(train_data.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "019d5679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12176"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing sklearn’s Countvectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#Creating a bag-of-words dictionary of words from the data\n",
    "bow_dictionary = CountVectorizer().fit(train_data['CLEAN_STORY'])\n",
    "\n",
    "#Total number of words in the bow_dictionary\n",
    "len(bow_dictionary.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "871ab506",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the bow_dictionary to create count vectors for the cleaned data.\n",
    "bow = bow_dictionary.transform(train_data['CLEAN_STORY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d5d021b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 12176)\n"
     ]
    }
   ],
   "source": [
    "#Printing the shape of the bag of words model\n",
    "print(bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "00f298eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing TfidfTransformer from sklearn\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "#Fitting the bag of words data to the TF-IDF transformer\n",
    "tfidf_transformer = TfidfTransformer().fit(bow)\n",
    "\n",
    "#Transforming the bag of words model to TF-IDF vectors\n",
    "storytfidf = tfidf_transformer.transform(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "81ae19ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 12176)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storytfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1c676290",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a Multinomial Naive Bayes Classifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#Fitting the training data to the classifier\n",
    "classifier = MultinomialNB().fit(storytfidf, train_data['SECTION'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1225135f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing and cleaning the test data\n",
    "test_data = pd.read_excel(\"Data_Test01.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ded00ebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2748, 1)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "99e8817a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=test_data[0:700]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6e01edec",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['CLEAN_STORY'] = test_data['STORY'].apply(text_cleaner)\n",
    "\n",
    "#Printing the cleaned data\n",
    "#print(test_data.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ed7db600",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the Pipeline module from sklearn\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4b7155c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the pipeline with necessary transformations and the required classifier\n",
    "pipe = Pipeline([\n",
    "('bow', CountVectorizer()),\n",
    "('tfidf', TfidfTransformer()),\n",
    "('classifier', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4925e256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('bow', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
       "                ('classifier', MultinomialNB())])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting the training data to the pipeline\n",
    "pipe.fit(train_data['CLEAN_STORY'], train_data['SECTION'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "471884df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting the SECTION\n",
    "test_preds_mnb = pipe.predict(test_data['CLEAN_STORY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "20b4f3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing the predictions to an excel sheet\n",
    "pd.DataFrame(test_preds_mnb, columns = ['SECTION']).to_excel(\"predictions.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72d7098",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
